<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-gb" xml:lang="en-gb">
<head>
<!--nomodify--> 
<meta name="style" content="cs" />
<title>University of Bristol - Computer Science Department - COMSM0018 - Applied Deep Learning</title>
</head>
<body>
<div id="wrap">
<h1>COMSM0018 - Applied Deep Learning</h1>
<img src="comsm0018.jpg" width="100%"></img>
<style>
td.blank  {vertical-align: center; horizontal-align: center; text-align: left; background: #f1f1f1;}
td.dima {vertical-align: center; horizontal-align: center; text-align: left; background: #FFE5B4;}
td.tilo   {vertical-align: center; horizontal-align: center; text-align: left; background: #ccffc1;}
</style>
    
<link rel="stylesheet" href="simple.css"> 
<a name="info"></a>     
<h2>Unit Information</h2>
   <p>Welcome to COMSM0018. The unit introduces the students to the latest deep architectures for learning linear and non-linear transformations of big data towards tasks such as classification and regression. The unit paves the path from understanding the fundamentals of convolutional and recurrent neural networks through to training and optimisation as well as evaluation of learnt outcomes. The unit's approach is hands-on, focusing on the 'how-to' while covering the basic theoretical foundations. For further general information, see <a
HREF="http://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=17/18&unitCode=COMSM0018" target="_blank">the syllabus for the unit</a>.

<h2> Staff</h2>

<table>
<tr><td class="dima"><a HREF="http://dimadamen.github.io">Dima Damen (DD)</a></td><td>office 3.12 MVB. <b>Unit Director</b></td></tr>
<tr><td class="tilo"><a HREF="http://www.cs.bris.ac.uk/~burghard/">Tilo Burghardt (TB)</a></td><td> office 3.42 MVB. </td></tr>
</table>
    
    <h2> Teaching Assistants</h2>

<p>Hazel Doughty (HD), Davide Moltisanti (DM), Miguel Fortiz (MF), Michael Wray (MW), Will Price (WP), Will Andrew (WA), Andre Poenaru (AP)</p>

<a name="materials"></a> 
<h2>Unit Materials</h2>

<table border="1" cellspacing="1" cellpadding="2" width="100%">
  <tr>
    <td><i>Wks</i></td> <td><i>Monday Sessions</i></td> <td><i>Thursday Sessions</i></td> <td><i>Labs</i></td> 
  </tr>
  <tr>
     <td>1</td> 
     <td class="tilo">        
       25/09/17, 11am, QB.F101 - LECTURE 1:<br/>        
       <b>BASICS OF ARTIFICIAL NEURAL NETWORKS</b><br/>        
       (Neural Networks, Perceptron, MLP, Cost Functions, Gradient Decent, Delta Rule)<hr/>         
       25/09/17, 1pm, QB.F101 - LECTURE 2:<br/>        
       <b>BASICS OF TRAINING DEEP FORWARD NETWORKS</b><br/>        
       (Activation Functions, Auto-Differentiation, Backpropagation)      
    </td> <td>-</td> <td>-</td> </tr>   <tr>   <td>2</td>         
  
    <td class="tilo">           
      02/10/17, 11am, QB.F101 - LECTURE 3:<br/>           
      <b>OPTIMISATION OF DEEP FORWARD NETWORKS</b><br/>           
      (SGD, Batch Optimisation, Newton's Method etc)<hr/>            
      02/10/17, 1pm, QB.F101 - LECTURE 4:<br/>           
      <b>PARAMETERS, REGULARISATION AND GENERALISATION</b><br/>           
      (Dropouts, Decay, Lx Regularisation, Generalisation etc)        
    </td> 
  
    <td class="dima">Convolutional Neural Networks (1hr)</td>         
  <td>Filters and convolutions (Home Work)<br/> Python reminder (Home Work)</td> </tr>   <tr>   <td>3</td> 
  <td class="dima">Practical1 (1hr)<br/>
Introduction to BC/ Intro to TensorFlow<br/>
Building your first model<br/>
Error rate monitoring (training/validation/testing)<br/>
       </td> <td class="dima">Recurrent Neural Networks (1hr)</td> <td>2hrs</td> </tr>
     <tr>   <td>4</td> <td class="dima">Practical2 (1hr)<br/> Batch-based training<br/>
Learning rate<br/>
Batch normalisation<br/>
Parameter intialisation</td> <td>-</td> <td>2hrs</td> </tr>
     <tr>   <td>5</td> <td>-</td> <td>-</td> <td>-</td> </tr>
    <tr>   <td>6</td> <td class="dima">Practical3 (1hr)<br/>Data augmentation<br/>
Debugging strategies<br/>
Adversal training<br/></td> <td>-</td> <td>2 hours</td></tr>
     <tr>   <td>7</td> <td class="dima">Practical4 (1hr) <br/> Baseline models<br/>
Changing your loss function<br/>
Adding/subtracting layers<br/>
Shallow/deep architectures<br/>
Hyperparameters</td> <td>-</td> <td>2 hours</td> </tr>
     <tr>   <td>8</td> <td colspan="2">READING WEEK</td> <td>2 hours (catchup lab)</td> </tr>
<tr>   <td>9</td> <td class="dima">From theory to practice - applications and success stories (1hr)</td> <td>-</td> <td>-</td> </tr>
<tr>   <td>10</td> <td>-</td> <td>1hr (collection of talks)</td> <td>Interum Presentations (3hrs)</td> </tr>
<tr>   <td>11</td> <td>-</td> <td>1hr (collection of talks)</td> <td>Interum Presentations (3hrs)</td> </tr>
<tr>   <td>12</td> <td>-</td> <td>1hr (collection of talks)</td> <td>-</td> </tr>

</table>

<h2>Assessment Details</h2>
<p>The student will undertake a challenge of replicating a state-of-the-art performance on a publicly available dataset using one of the deep architectures discussed on the unit. The unit will have three assessments
    <ol><li> Lab portfolio (20%)
    </li>
    <li> Individual Oral Presentation (5 minutes) (20%) [Wk10 - Wk11]</li>
    <li> Group (up to 3) Project - assessed by a final report (60%) [Wk13] </li>
    </ol>
</p>
<h2>Textbook</h2>
<p>Goodfellow et al (2016). Deep Learning. MIT Press</p>
</div>
</body>
</html> 
